import streamlit as st
from langchain.llms import LlamaCpp
from langchain import PromptTemplate
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

st.title("Summariser App")
st.write("#### Generated by an Open Source LLM model Llama 2")

prompt = PromptTemplate(
    input_variables=["input"],
    template="Please summarize the following text: {input}",
)

#Â Loading the Llama 2's LLM
def load_llm():
    # We instantiate the callback with a streaming stdout handler
    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])   

    llm:LlamaCpp  = LlamaCpp(
        model_path="./models/llama-2-7b-chat.ggmlv3.q8_0.bin",
        temperature=0.7,
        max_tokens=2000,
        top_p=1,
        callback_manager=callback_manager, 
        verbose=True,
        n_ctx=2000
    )
    return llm

def get_summary(question):
    llm = load_llm()
    final_prompt = prompt.format(input=question)
    with st.spinner("Summarising the content.."):
        st.info(llm(final_prompt))
        st.balloons()


with st.form("summary_form"):
    text = st.text_area("Paste the text here to summarise it:", value="", max_chars=5000)

    submitted = st.form_submit_button("Submit")
    if submitted:
        get_summary(text)